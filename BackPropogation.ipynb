{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BackPropogation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhELhjcwhThl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    The sigmoid activation function.\n",
        "    \"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"\n",
        "    Derivative of the sigmoid function.\n",
        "    \"\"\"\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "def cost_function(network, test_data, onehot=True):\n",
        "    \"\"\"\n",
        "    Целевая функция с усреднением  (1/n * (y_hat - y) ^ 2).\n",
        "    \"\"\"\n",
        "    c = 0\n",
        "    for example, y in test_data:\n",
        "        # если ожидаемые значения представлены не в формате onehot, то переформатируем их \n",
        "        if not onehot:\n",
        "            y = np.eye(3, 1, k =- int(y))\n",
        "        yhat = network.feedforward(example)\n",
        "        c += np.sum((y - yhat) ** 2)\n",
        "    return c / len(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulzNc_kSl3EA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network:\n",
        "    def __init__(self, shape, activation_function, activation_function_derivative, output=True,):\n",
        "        \"\"\"\n",
        "        Список ``shape`` содержит количество нейронов в соответствующих слоях\n",
        "        нейронной сети. К примеру, если бы этот лист выглядел как [2, 3, 1],\n",
        "        то мы бы получили трёхслойную нейросеть, с двумя нейронами в первом\n",
        "        (входном), тремя нейронами во втором (промежуточном) и одним нейроном\n",
        "        в третьем (выходном, внешнем) слое. Смещения и веса для нейронных сетей\n",
        "        инициализируются случайными значениями, подчиняющимися стандартному нормальному\n",
        "        распределению. Обратите внимание, что первый слой подразумевается слоем, \n",
        "        принимающим входные данные, поэтому мы не будем добавлять к нему смещение \n",
        "        (делать это не принято, поскольку смещения используются только при \n",
        "        вычислении выходных значений нейронов последующих слоёв)\n",
        "        \"\"\"\n",
        "\n",
        "        self.layers = len(shape)\n",
        "        self.shape = shape\n",
        "        self.biases = [np.random.randn(y, 1) for y in shape[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(shape[:-1], shape[1:])]\n",
        "        self.output = output\n",
        "        self.activation_function = activation_function\n",
        "        self.activation_function_derivative = activation_function_derivative\n",
        "\n",
        "    def feedforward(self, input_matrix):\n",
        "        \"\"\"\n",
        "        ``input_matrix`` - матрица входов размерностью (n, m)\n",
        "        Вычислить и вернуть выходную активацию нейронной сети\n",
        "        при получении ``input_matrix`` на входе (бывшее forward_pass).\n",
        "        Возвращает матрицу (n, 1)\n",
        "        \"\"\"\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            # weigts - (массив матриц)\n",
        "            input_matrix = self.activation_function(np.dot(w, input_matrix) + b)\n",
        "        return input_matrix\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, alpha):\n",
        "        \"\"\"\n",
        "        Обновить веса и смещения нейронной сети, сделав шаг градиентного\n",
        "        спуска на основе алгоритма обратного распространения ошибки, примененного\n",
        "        к одному mini batch.\n",
        "        ``mini_batch`` - список кортежей вида ``(x, y)`` x - вход, y - желаемый выход,\n",
        "        ``alpha`` - величина шага (learning rate).\n",
        "        \"\"\"\n",
        "        # значения dJ/db для каждого слоя\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        # значения dJ/dw (ошибки) для каждого слоя\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # для каждого примера из батча применяем бек пропогейшн\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "            \n",
        "        eps = alpha / len(mini_batch)\n",
        "        # обновляем параметры сети\n",
        "        self.weights = [w - eps * nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases  = [b - eps * nb for b, nb in zip(self.biases,  nabla_b)]\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"\n",
        "        Вернуть количество тестовых примеров, для которых нейронная сеть\n",
        "        возвращает правильный ответ. Обратите внимание: подразумевается,\n",
        "        что выход нейронной сети - это индекс, указывающий, какой из нейронов\n",
        "        последнего слоя имеет наибольшую активацию. (случай для формата one-hot)\n",
        "        \"\"\"\n",
        "        # кортеж полученных и ожидаемых значений для каждого примера тестовой выборки\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
        "        # возвращаем количество верно угаданных целевых переменных\n",
        "        return sum(int(x == y) for (x, y) in test_results)\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, alpha, test_data=None):\n",
        "        \"\"\"\n",
        "        Обучить нейронную сеть, используя алгоритм стохастического\n",
        "        (mini-batch) градиентного спуска. \n",
        "        ``training_data`` - лист кортежей вида ``(x, y)``, где \n",
        "        x - вход обучающего примера, y - желаемый выход (в формате one-hot). \n",
        "        Роль остальных обязательных параметров должна быть понятна из их названия.\n",
        "        Если предоставлен опциональный аргумент ``test_data``, \n",
        "        то после каждой эпохи обучения сеть будет протестирована на этих данных \n",
        "        и промежуточный результат обучения будет выведен в консоль. \n",
        "        ``test_data`` -- это список кортежей из входных данных \n",
        "        и номеров правильных классов примеров (т.е. argmax(y),\n",
        "        если y -- набор ответов в той же форме, что и в тренировочных данных).\n",
        "        Тестирование полезно для мониторинга процесса обучения,\n",
        "        но может существенно замедлить работу программы.\n",
        "        \"\"\"\n",
        "\n",
        "        if test_data is not None: \n",
        "            n_test = len(test_data)\n",
        "\n",
        "        n = len(training_data)\n",
        "        success_tests = 0\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            # выбираем несолько батчей, покрывающих всю тренировочную выборку\n",
        "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "            # обновляем параметры сети (веса, биасы) бекпропогейшном для каждого из батчей\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, alpha)\n",
        "            if test_data is not None and self.output:\n",
        "                success_tests = self.evaluate(test_data)\n",
        "                print(f'Эпоха {j}: {success_tests} / {n_test}')\n",
        "            elif self.output:\n",
        "                pass\n",
        "                # print(f'Эпоха {j} завершена')\n",
        "        # в случае наличия тестовой выборки возвращаем количество правильных активаций сети\n",
        "        if test_data is not None:\n",
        "            return success_tests / n_test\n",
        "    \n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        \"\"\"\n",
        "        Возвращает вектор частных производных (\\partial C_x) / (\\partial a) \n",
        "        целевой функции по активациям выходного слоя. подходит для квадратичной функции \n",
        "        \"\"\"\n",
        "        return (output_activations-y)\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"\n",
        "        Возвращает кортеж ``(nabla_b, nabla_w)`` -- градиент целевой функции по всем параметрам сети.\n",
        "        ``nabla_b`` и ``nabla_w`` -- послойные списки массивов ndarray,\n",
        "        такие же, как self.biases и self.weights соответственно.\n",
        "        \"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # прямое распространение (forward pass)\n",
        "        activations = [x]\n",
        "        summatories = []\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            summatories.append(np.dot(w, activations[-1]) + b)\n",
        "            activation = self.activation_function(summatories[-1])\n",
        "            activations.append(activation)\n",
        "\n",
        "        # обратное распространение (backward pass)\n",
        "        \n",
        "        # ошибка для выходного слоя\n",
        "        delta = self.cost_derivative(activations[-1], y) * self.activation_function_derivative(summatories[-1])\n",
        "        # производная J по биасам выходного слоя\n",
        "        nabla_b[-1] = delta\n",
        "        # производная J по весам выходного слоя\n",
        "        nabla_w[-1] = delta.dot(activations[-2].T)\n",
        "\n",
        "        # Здесь l = 1 означает последний слой, l = 2 - предпоследний и так далее.  \n",
        "        for l in range(2, self.layers):\n",
        "            derivative = self.activation_function_derivative(summatories[-l])\n",
        "            # суммарные ошибки следующих слоев\n",
        "            total_deltas = self.weights[-l + 1].T.dot(delta)\n",
        "            # ошибка на слое L-l\n",
        "            delta = derivative * total_deltas\n",
        "            # производная J по смещениям L-l-го слоя\n",
        "            nabla_b[-l] = delta\n",
        "            # производная J по весам L-l-го слоя\n",
        "            nabla_w[-l] = delta.dot(activations[-l - 1].T) \n",
        "        return nabla_b, nabla_w\n",
        "\n",
        "    def test(self, test_data):\n",
        "        for (x, y) in test_data:\n",
        "            result = np.argmax(self.feedforward(x))\n",
        "            print(f'result for {list(x)} is {result} should be: {y}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKY8n9F8PYoS",
        "colab_type": "code",
        "outputId": "69b017f7-7cce-48c7-df26-a006e0243745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "training_data = [(np.array([[0], [0]]), 0), \n",
        "                 (np.array([[0], [1]]), 1), \n",
        "                 (np.array([[1], [0]]), 1), \n",
        "                 (np.array([[1], [1]]), 0)]\n",
        "# XOR\n",
        "network = Network([2, 2, 2], sigmoid, sigmoid_prime)\n",
        "network.SGD(training_data, 100000, 4, 0.01)\n",
        "network.test(training_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "result for [array([0]), array([1])] is 0 should be: 1\n",
            "result for [array([1]), array([1])] is 1 should be: 0\n",
            "result for [array([1]), array([0])] is 1 should be: 1\n",
            "result for [array([0]), array([0])] is 1 should be: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r2r0Zn16rB4",
        "colab_type": "code",
        "outputId": "82844afc-17ae-499f-fb6f-4c66f7ca7b3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "network = Network([3,2,1],sigmoid, sigmoid_prime)\n",
        "network.biases = [np.array([[0], [0]]), np.array([[0]])]\n",
        "network.weights = [np.array([[0.2, 0.9, 0.6], [0.2, 0.3,  0.7]]), np.array([[0.2, 0.5]])]\n",
        "\n",
        "x = np.array([[15], [5], [15]])\n",
        "y = np.array([[1]])\n",
        "nabla_b, nabla_w = network.backprop(x, y)\n",
        "nabla_w"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape activ (3, 1)\n",
            "shape w (2, 3)\n",
            "shape activ (2, 1)\n",
            "shape w (1, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-1.50641840e-08, -5.02139468e-09, -1.50641840e-08],\n",
              "        [-1.68782392e-07, -5.62607975e-08, -1.68782392e-07]]),\n",
              " array([[-0.07356705, -0.07356703]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FP4zFNTN99c",
        "colab_type": "code",
        "outputId": "5f403ab7-1412-46c5-c744-53169f97c96f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(sigmoid(0.9 * 0.2) - 1) * 0.2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.09102422152528399"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AucWHENDxQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network = Network([3,2], sigmoid, sigmoid_prime)\n",
        "network.biases = [np.array([[-1], [-1]])]\n",
        "network.weights = [np.array([[-1,1,-1], [1,-1,1]])]\n",
        "\n",
        "x = np.array([[1], [2], [3]])\n",
        "y = np.array([[0], [1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvhTBholwky5",
        "colab_type": "code",
        "outputId": "609319eb-92d4-4aed-942e-6558693332b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "nabla_b, nabla_w = network.backprop(x, y)\n",
        "print(nabla_b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[ 0.00214254],\n",
            "       [-0.05287709]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFcZxU-jww6f",
        "colab_type": "code",
        "outputId": "5e7ca611-f221-4bcf-8aae-dc144c88b510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(nabla_w[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.00214254 -0.05287709]\n",
            " [ 0.00428509 -0.10575419]\n",
            " [ 0.00642763 -0.15863128]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}